{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Dormouse's story\n",
    "\n",
    "html_doc = \"\"\"\n",
    "<html>\n",
    "<head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were</p>\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;</p>\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   The Dormouse's story\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p class=\"title\">\n",
      "   <b>\n",
      "    The Dormouse's story\n",
      "   </b>\n",
      "  </p>\n",
      "  <p class=\"story\">\n",
      "   Once upon a time there were three little sisters; and their names were\n",
      "  </p>\n",
      "  <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n",
      "   Elsie\n",
      "  </a>\n",
      "  ,\n",
      "  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n",
      "   Lacie\n",
      "  </a>\n",
      "  and\n",
      "  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\n",
      "   Tillie\n",
      "  </a>\n",
      "  ;\n",
      " </body>\n",
      "</html>\n",
      "and they lived at the bottom of a well.\n",
      "<p class=\"story\">\n",
      " ...\n",
      "</p>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#显示BeautifulSoup 的对象,\n",
    "from bs4 import BeautifulSoup\n",
    "#以文件方式读入html，构造Soup 的对象\n",
    "soup = BeautifulSoup(open(\"index.html\"))\n",
    "#以字符串方式读入，构造Soup 的对象\n",
    "soup = BeautifulSoup(html_doc,'html.parser')\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取soup对象的类型\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### soup.select命令的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>The Dormouse's story</title>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 选择所有title标签\n",
    "soup.select(\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 选择body标签下的所有a标签\n",
    "soup.select(\"body a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选择id=link1后的所有兄弟节点标签\n",
    "soup.select(\"#link1 ~.sister\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选择a标签，其类属性为sister的标签\n",
    "soup.select(\"a.sister\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取a标签中具有href属性的标签\n",
    "soup.select('a[href]') \n",
    "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
    "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选择a标签，其id属性为link1的标签\n",
    "soup.select(\"a#link1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>The Dormouse's story</title>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取title标签\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'title'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取title标签名称\n",
    "soup.title.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Dormouse's story\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取title标签的内容\n",
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p class=\"title\"><b>The Dormouse's story</b></p>\n"
     ]
    }
   ],
   "source": [
    "# 获取p标签\n",
    "print(soup.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<b>The Dormouse's story</b>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.p.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Dormouse's story\n",
      "\n",
      "The Dormouse's story\n",
      "Once upon a time there were three little sisters; and their names were\n",
      "Elsie,\n",
      "Lacie and\n",
      "Tillie;\n",
      "and they lived at the bottom of a well.\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#从HTML文档中获取所有文字内容:\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取id='link3'的标签\n",
    "soup.find(id=\"link3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['title']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取p标签class属性\n",
    "soup.p['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://example.com/elsie'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.a['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取第1个a标签内容\n",
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'href': 'http://example.com/elsie', 'class': ['sister'], 'id': 'link1'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取标签a的属性\n",
    "soup.a.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Elsie'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取标签a的文本内容\n",
    "soup.a.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sister']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取class属性\n",
    "soup.a['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
       " <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取所有的a标签的内容\n",
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://example.com/elsie\n",
      "http://example.com/lacie\n",
      "http://example.com/tillie\n"
     ]
    }
   ],
   "source": [
    "# 获取所有的a标签的链接\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n",
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "req = requests.get(\"http://www.weather.com.cn/\") #使用get方法中国天气网,返回Response对象\n",
    "print(type(req))\n",
    "print(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<list_iterator object at 0x00000223709C6780>\n",
      "<body>\n",
      "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
      "<p class=\"story\">Once upon a time there were three little sisters; and their names were</p>\n",
      "<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
      "<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a> and\n",
      "<a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>;</body>\n"
     ]
    }
   ],
   "source": [
    "#通过soup.a.parent就可以获取父节点的信息\n",
    "print(soup.p.children)\n",
    "print(soup.a.parent)\n",
    "#for i,child in enumerate(soup.p.children):\n",
    "    #print(i,child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object next_siblings at 0x000002237095F4C0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.a.next_siblings #获取后面的兄弟节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reqeusts发送post请求\n",
    "import requests\n",
    "url = 'http://www.baidu.com/post'\n",
    "param = {'key1': 'value1', 'key2': 'value2'}\n",
    "req1 = requests.post(url,params=param)    #表单格式\n",
    "req2=requests.post(url,json=param)        #json格式数据\n",
    "#print(req1.text)\n",
    "#print(req2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#爬取网页异常处理的通用框架\n",
    "import requests\n",
    "url = 'http://www.baidu.com'\n",
    "\n",
    "def requests_caputure():\n",
    "    try:\n",
    "        r=requests.get(url,timeout=30)#请求超时时间为30秒\n",
    "        r.raise_for_status()#如果状态不是200，则引发异常\n",
    "        r.encoding=r.apparent_encoding #配置编码\n",
    "        return r.text\n",
    "    except:\n",
    "        return \"产生异常\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    response=requests_caputure()\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.baidu.com/s?wd=Python\n",
      "420646\n"
     ]
    }
   ],
   "source": [
    "#百度搜索关键字提交\n",
    "import requests\n",
    "Keyword = \"Python\"\n",
    "try:\n",
    "    kw = {'wd':Keyword}\n",
    "    r = requests.get(\"http://www.baidu.com/s\",params=kw)\n",
    "    print(r.request.url)\n",
    "    r.raise_for_status()\n",
    "    print(len(r.text))\n",
    "    #print(r.text[:1000])\n",
    "except:\n",
    "    print(\"爬取失败\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "#示例：使用requests模块发起一个get请求，获取响应对象\n",
    "import requests #导入requests模块\n",
    "\n",
    "#指定请求的url地址\n",
    "url = 'http://www.baidu.com'\n",
    "#使用requests模块的get函数根据指定的url发起一个get请求，get函数返回一个响应对象\n",
    "response = requests.get(url)\n",
    "#响应对象信息\n",
    "print(type( response)) #输出响应对象类型\n",
    "print(response.status_code)#响应状态码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "响应内容编码：ISO-8859-1\n",
      "响应对象的头信息: {'Cache-Control': 'private, no-cache, no-store, proxy-revalidate, no-transform', 'Connection': 'Keep-Alive', 'Content-Encoding': 'gzip', 'Content-Type': 'text/html', 'Date': 'Wed, 26 Jun 2019 10:09:13 GMT', 'Last-Modified': 'Mon, 23 Jan 2017 13:27:56 GMT', 'Pragma': 'no-cache', 'Server': 'bfe/1.0.8.18', 'Set-Cookie': 'BDORZ=27315; max-age=86400; domain=.baidu.com; path=/', 'Transfer-Encoding': 'chunked'}\n",
      "响应内容: <!DOCTYPE html>\r\n",
      "<!--STATUS OK--><html> <head><meta http-equiv=content-type content=text/html;charset=utf-8><meta http-equiv=X-UA-Compatible content=IE=Edge><meta content=always name=referrer><link rel=stylesheet type=text/css href=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css><title>ç¾åº¦ä¸ä¸ï¼ä½ å°±ç¥é</title></head> <body link=#0000cc> <div id=wrapper> <div id=head> <div class=head_wrapper> <div class=s_form> <div class=s_form_wrapper> <div id=lg> <img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129> </div> <form id=form name=f action=//www.baidu.com/s class=fm> <input type=hidden name=bdorz_come value=1> <input type=hidden name=ie value=utf-8> <input type=hidden name=f value=8> <input type=hidden name=rsv_bp value=1> <input type=hidden name=rsv_idx value=1> <input type=hidden name=tn value=baidu><span class=\"bg s_ipt_wr\"><input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus></span><span class=\"bg s_btn_wr\"><input type=submit id=su value=ç¾åº¦ä¸ä¸ class=\"bg s_btn\"></span> </form> </div> </div> <div id=u1> <a href=http://news.baidu.com name=tj_trnews class=mnav>æ°é»</a> <a href=http://www.hao123.com name=tj_trhao123 class=mnav>hao123</a> <a href=http://map.baidu.com name=tj_trmap class=mnav>å°å¾</a> <a href=http://v.baidu.com name=tj_trvideo class=mnav>è§é¢</a> <a href=http://tieba.baidu.com name=tj_trtieba class=mnav>è´´å§</a> <noscript> <a href=http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb>ç»å½</a> </noscript> <script>document.write('<a href=\"http://www.baidu.com/bdorz/login.gif?login&tpl=mn&u='+ encodeURIComponent(window.location.href+ (window.location.search === \"\" ? \"?\" : \"&\")+ \"bdorz_come=1\")+ '\" name=\"tj_login\" class=\"lb\">ç»å½</a>');</script> <a href=//www.baidu.com/more/ name=tj_briicon class=bri style=\"display: block;\">æ´å¤äº§å</a> </div> </div> </div> <div id=ftCon> <div id=ftConw> <p id=lh> <a href=http://home.baidu.com>å",
      "³äºç¾åº¦</a> <a href=http://ir.baidu.com>About Baidu</a> </p> <p id=cp>&copy;2017&nbsp;Baidu&nbsp;<a href=http://www.baidu.com/duty/>ä½¿ç¨ç¾åº¦åå¿",
      "è¯»</a>&nbsp; <a href=http://jianyi.baidu.com/ class=cp-feedback>æè§åé¦</a>&nbsp;äº¬ICPè¯030173å·&nbsp; <img src=//www.baidu.com/img/gs.gif> </p> </div> </div> </div> </body> </html>\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#获取响应内容的编码格式,\n",
    "print('响应内容编码：'+response.encoding)\n",
    "#获取响应对象的响应头信息：\n",
    "print(\"响应对象的头信息:\",response.headers)\n",
    "#获取字符串形式的响应内容，即看到的HTML内容\n",
    "print(\"响应内容:\",response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "#需求：使用requests模块想百度首页面发起一个get请求，获取响应对象\n",
    "import requests\n",
    "if __name__ == \"__main__\":\n",
    "    #指定请求的url地址\n",
    "    url = 'http://www.baidu.com'\n",
    "    #使用requests模块的get函数根据指定的url发起一个get请求，get函数返回一个响应对象\n",
    "    response = requests.get(url)\n",
    "    #打印响应对象（结果：响应对象类型和响应状态码）\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "r = requests.get(\"http://www.weather.com.cn/\")\n",
    "r.encoding = \"utf-8\"    #为了简化代码，没有考虑异常情况。\n",
    "soup = BeautifulSoup(r.text)    #soup就是一个BeautifulSoup对象。\n",
    "type(soup)\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "爬虫_百度搜索\n"
     ]
    }
   ],
   "source": [
    "\"\"\"根据上面说的步骤来完成一个简单的爬虫程序\"\"\"\n",
    "\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "target_url = 'http://www.baidu.com/s?wd=爬虫'\n",
    "\n",
    "# 第一步 发起一个GET请求\n",
    "res = requests.get(target_url)\n",
    "\n",
    "# 第二步 提取HTML并解析想获取的数据 比如获取 title\n",
    "soup = BeautifulSoup(res.text, \"lxml\")\n",
    "# 输出 soup.title.text\n",
    "title = soup.title.text\n",
    "print(title)\n",
    "# 第三步 持久化 比如保存到本地\n",
    "#with open('title.txt', 'w') as fp:\n",
    "#    fp.write(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请输入城市名称(按q/Q键退出):北京\n",
      "\t\t\t 北京一周天气预报\n",
      "    日期    \t   天气   \t  最高温度  \t  最低温度  \t   风级   \n",
      "  1日（今天）  \t   多云   \t        \t  22    \t  <3级   \n",
      "  2日（明天）  \t  多云转晴  \t  35    \t  23    \t  <3级   \n",
      "  3日（后天）  \t   晴    \t  36    \t  24    \t  <3级   \n",
      "  4日（周四）  \t  晴转多云  \t  38    \t  26    \t  <3级   \n",
      "  5日（周五）  \t 多云转雷阵雨 \t  34    \t  24    \t<3级转3-4级\n",
      "  6日（周六）  \t   多云   \t  33    \t  23    \t  <3级   \n",
      "  7日（周日）  \t   多云   \t  34    \t  23    \t  <3级   \n",
      "请输入城市名称(按q/Q键退出):上海\n",
      "\t\t\t 上海一周天气预报\n",
      "    日期    \t   天气   \t  最高温度  \t  最低温度  \t   风级   \n",
      "  1日（今天）  \t   小雨   \t        \t  23    \t  <3级   \n",
      "  2日（明天）  \t 小雨转中雨  \t  27    \t  22    \t  <3级   \n",
      "  3日（后天）  \t   小雨   \t  26    \t  23    \t<3级转3-4级\n",
      "  4日（周四）  \t   小雨   \t  27    \t  21    \t3-4级转<3级\n",
      "  5日（周五）  \t   中雨   \t  26    \t  21    \t<3级转3-4级\n",
      "  6日（周六）  \t   小雨   \t  26    \t  20    \t  3-4级  \n",
      "  7日（周日）  \t 小雨转多云  \t  26    \t  22    \t3-4级转<3级\n",
      "请输入城市名称(按q/Q键退出):Q\n",
      "程序结束...\n"
     ]
    }
   ],
   "source": [
    "#综合案例1：查询中国天气网7天预报数据\n",
    "#参考URL: https://blog.csdn.net/xunkhun/article/details/79266283\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#from pyecharts import Bar\n",
    "\n",
    "#定义天气城市代码\n",
    "cityCodeDict={\n",
    "        \"北京\":101010100,\"上海\":101020100,\"天津\":101030100,\"重庆\":101040100,\"哈尔滨\":101050101,\n",
    "        \"长春\":101060101,\"沈阳\":101070101,\"呼和浩特\":101080101,\"石家庄\":101090101,\"太原\":101100101,\n",
    "        \"西安\":101110101,\"济南\":101120101,\"乌鲁木齐\":101130101,\"拉萨\":101140101,\"西宁\":101150101,\n",
    "        \"兰州\":101160101,\"银川\":101170101,\"郑州\":101180101,\"南京\":101190101,\"武汉\":101200101,\n",
    "        \"杭州\":101210101,\"合肥\":101220101,\"福州\":101230101,\"南昌\":101240101,\"长沙\":101250101,\n",
    "        \"贵阳\":101260101,\"成都\":101270101,\"广州\":101280101,\"昆明\":101290101,\"南宁\":101300101,\n",
    "        \"海口\":101310101,\"香港\":101320101,\"澳门\":101330101,\"台北\":101340102\n",
    "        }\n",
    "\n",
    "def getHTMLText(url,timeout = 30):\n",
    "    try:\n",
    "        r = requests.get(url, timeout = 30)       #用requests抓取网页信息\n",
    "        r.raise_for_status()                      #可以让程序产生异常时停止程序\n",
    "        r.encoding = r.apparent_encoding\n",
    "        return r.text\n",
    "    except:\n",
    "        return '产生异常'\n",
    "    \n",
    " \n",
    "def get_data(html):\n",
    "    final_list = []\n",
    "    soup = BeautifulSoup(html,'html.parser')       #html.parser表示用BeautifulSoup库解析网页\n",
    "    body  = soup.body                              #取body结构\n",
    "    data = body.find('div',{'id':'7d'})\n",
    "    ul = data.find('ul')\n",
    "    lis = ul.find_all('li')\n",
    " \n",
    " \n",
    "    for day in lis:\n",
    "        temp_list = []\n",
    "        \n",
    "        date = day.find('h1').string             #找到日期\n",
    "        temp_list.append(date)     \n",
    "    \n",
    "        info = day.find_all('p')                 #找到所有的p标签\n",
    "        temp_list.append(info[0].string)\n",
    "    \n",
    "        if info[1].find('span') is None:          #找到p标签中的第二个值'span'标签——最高温度\n",
    "            temperature_highest = ' '             #用一个判断是否有最高温度\n",
    "        else:\n",
    "            temperature_highest = info[1].find('span').string\n",
    "            temperature_highest = temperature_highest.replace('℃',' ')\n",
    "            \n",
    "        if info[1].find('i') is None:              #找到p标签中的第二个值'i'标签——最低温度\n",
    "            temperature_lowest = ' '               #用一个判断是否有最低温度\n",
    "        else:\n",
    "            temperature_lowest = info[1].find('i').string\n",
    "            temperature_lowest = temperature_lowest.replace('℃',' ')\n",
    "            \n",
    "        temp_list.append(temperature_highest)       #将最高气温添加到temp_list中\n",
    "        temp_list.append(temperature_lowest)        #将最低气温添加到temp_list中\n",
    "    \n",
    "        wind_scale = info[2].find('i').string      #找到p标签的第三个值'i'标签——风级，添加到temp_list中\n",
    "        temp_list.append(wind_scale)\n",
    "    \n",
    "        final_list.append(temp_list)              #将temp_list列表添加到final_list列表中\n",
    "    return final_list\n",
    "    \n",
    "\n",
    "#用format()将结果打印输出\n",
    "def print_data(final_list,num,cityName):\n",
    "    print(\"\\t\\t\\t {}一周天气预报\".format(cityName))\n",
    "    print(\"{:^10}\\t{:^8}\\t{:^8}\\t{:^8}\\t{:^8}\".format('日期','天气','最高温度','最低温度','风级'))\n",
    "    for i in range(num):    \n",
    "        items= final_list[i]\n",
    "        print(\"{:^10}\\t{:^8}\\t{:^8}\\t{:^8}\\t{:^8}\".format(items[0],items[1],items[2],items[3],items[4]))\n",
    "        \n",
    "#定义主函数main()\n",
    "def main():\n",
    "        \n",
    "    while (True):\n",
    "        try:\n",
    "            cityName = input('请输入城市名称(按q/Q键退出):')\n",
    "            if cityName == 'q' or cityName == 'Q':\n",
    "                print(\"程序结束...\")\n",
    "                break\n",
    "            cityCode = cityCodeDict[cityName]  #得到城市代码\n",
    "            url = 'http://www.weather.com.cn/weather/%d.shtml' % cityCode  #得到城市天气网址\n",
    "            #print(url)\n",
    "            html = getHTMLText(url)\n",
    "            final_list = get_data(html)\n",
    "            print_data(final_list,7,cityName)  #输出天气数据\n",
    "        except:\n",
    "            print('未查到%s城市，请重新输入：'%cityName)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这个文件也是读7天天气预报的\n",
    "import csv\n",
    "import urllib.request\n",
    "from  bs4 import BeautifulSoup\n",
    " \n",
    "url = \"http://www.weather.com.cn/weather/101270101.shtml\"\n",
    "header = (\"User-Agent\",\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36\")  # 设置头部信息\n",
    "opener = urllib.request.build_opener()  # 修改头部信息\n",
    "opener.addheaders = [header]         #修改头部信息\n",
    "request = urllib.request.Request(url)   # 制作请求\n",
    "response = urllib.request.urlopen(request)   #  得到请求的应答包\n",
    "html = response.read()   #将应答包里面的内容读取出来\n",
    "html = html.decode('utf-8')    # 使用utf-8进行编码，不重新编码就会成乱码\n",
    " \n",
    " \n",
    "final = []   #初始化一个空的list，我们为将最终的的数据保存到list\n",
    "bs = BeautifulSoup(html,\"html.parser\")   # 创建BeautifulSoup对象\n",
    "body = bs.body  # 获取body部分\n",
    "data = body.find('div',{'id':'7d'})  # 找到id为7d的div\n",
    "ul = data.find('ul')  # 获取ul部分\n",
    "li = ul.find_all('li')  # 获取所有的li\n",
    "# print (li)\n",
    " \n",
    "i = 0\n",
    "for day in li:  # 对每个li标签中的内容进行遍历\n",
    "    if i < 7:\n",
    "        temp = []\n",
    "        date = day.find('h1').string # 找到日期\n",
    "#         print (date)\n",
    "        temp.append(date)  # 添加到temp中\n",
    "    #     print (temp)\n",
    "        inf = day.find_all('p')  # 找到li中的所有p标签\n",
    "    #     print(inf)\n",
    "    #     print (inf[0])\n",
    "        temp.append(inf[0].string)  # 第一个p标签中的内容（天气状况）加到temp中\n",
    "        if inf[1].find('span') is None:\n",
    "            temperature_highest = None # 天气预报可能没有当天的最高气温（到了傍晚，就是这样），需要加个判断语句,来输出最低气温\n",
    "        else:\n",
    "            temperature_highest = inf[1].find('span').string # 找到最高温度\n",
    "            temperature_highest = temperature_highest.replace('℃', '') # 到了晚上网站会变，最高温度后面也有个℃\n",
    "        temperature_lowest = inf[1].find('i').string  #找到最低温度\n",
    "        temperature_lowest = temperature_lowest.replace('℃', '')  # # 最低温度后面有个℃，去掉这个符号\n",
    "        temp.append(temperature_highest)\n",
    "        temp.append(temperature_lowest)\n",
    "        final.append(temp)\n",
    "        i = i +1\n",
    "        \n",
    "# print(final)\n",
    " \n",
    "with open('weather.csv', 'a', errors='ignore', newline='') as f:\n",
    "            f_csv = csv.writer(f)\n",
    "            f_csv.writerows(final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 综合案例2：爬取电影票房数据\n",
    "\n",
    "# 数据抓取&解析\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "r = requests.get('http://www.cbooo.cn/year?year=2018')\n",
    "r.encoding = \"utf-8\"\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "# 数据提取\n",
    "movies_table = soup.find_all('table', {'id': \"tbContent\"})[0]\n",
    "movies = movies_table.find_all('tr')\n",
    "# 提取影片名和影片链接\n",
    "names = [ tr.find_all('td')[0].a.get('title')  for tr in movies[1:]]\n",
    "urls = [ tr.find_all('td')[0].a.get('href')  for tr in movies[1:]]\n",
    "# 提取电影类型\n",
    "types = [ tr.find_all('td')[1].string  for tr in movies[1:]]\n",
    "# 提取电影票房\n",
    "boxoffice = [ tr.find_all('td')[2].string  for tr in movies[1:]]\n",
    "#转为数据框&保存为csv文件\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'影片名': names,'类型': types,\n",
    "'总票房（万元）': boxoffice, '链接' : urls\n",
    "                   })\n",
    "filename = \"movie-boxoffice.csv\"\n",
    "df.to_csv(filename, encoding=\"utf_8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "[['厦门精图信息技术有限公司', 'Python开发工程师', '不限经验', '不限学历', '6000-10000元/月']]\n",
      "[['厦门精图信息技术有限公司', 'Python开发工程师', '不限经验', '不限学历', '6000-10000元/月'], ['上海红意坊市场营销策划事务所', 'python后端', '1-3年', '本科以上', '10000-15000元']]\n",
      "[['厦门精图信息技术有限公司', 'Python开发工程师', '不限经验', '不限学历', '6000-10000元/月'], ['上海红意坊市场营销策划事务所', 'python后端', '1-3年', '本科以上', '10000-15000元'], ['匠思崛（厦门）信息科技有限公司', 'python实习', '不限经验', '不限学历', '2千-4千']]\n",
      "[['厦门精图信息技术有限公司', 'Python开发工程师', '不限经验', '不限学历', '6000-10000元/月'], ['上海红意坊市场营销策划事务所', 'python后端', '1-3年', '本科以上', '10000-15000元'], ['匠思崛（厦门）信息科技有限公司', 'python实习', '不限经验', '不限学历', '2千-4千'], ['厦门金投缘信息科技有限公司', 'Python数据分析', '1-3年', '本科以上', '10000-20000元']]\n",
      "[['厦门精图信息技术有限公司', 'Python开发工程师', '不限经验', '不限学历', '6000-10000元/月'], ['上海红意坊市场营销策划事务所', 'python后端', '1-3年', '本科以上', '10000-15000元'], ['匠思崛（厦门）信息科技有限公司', 'python实习', '不限经验', '不限学历', '2千-4千'], ['厦门金投缘信息科技有限公司', 'Python数据分析', '1-3年', '本科以上', '10000-20000元'], ['厦门北星博辉科技有限公司', 'Python开发工程师', '1-3年', '本科以上', '8000-12000']]\n",
      "[['厦门精图信息技术有限公司', 'Python开发工程师', '不限经验', '不限学历', '6000-10000元/月'], ['上海红意坊市场营销策划事务所', 'python后端', '1-3年', '本科以上', '10000-15000元'], ['匠思崛（厦门）信息科技有限公司', 'python实习', '不限经验', '不限学历', '2千-4千'], ['厦门金投缘信息科技有限公司', 'Python数据分析', '1-3年', '本科以上', '10000-20000元'], ['厦门北星博辉科技有限公司', 'Python开发工程师', '1-3年', '本科以上', '8000-12000'], ['厦门匹夫有道信息技术有限公司', 'Python开发+双休', '不限经验', '大专以上', '3K-5K']]\n",
      "[['厦门精图信息技术有限公司', 'Python开发工程师', '不限经验', '不限学历', '6000-10000元/月'], ['上海红意坊市场营销策划事务所', 'python后端', '1-3年', '本科以上', '10000-15000元'], ['匠思崛（厦门）信息科技有限公司', 'python实习', '不限经验', '不限学历', '2千-4千'], ['厦门金投缘信息科技有限公司', 'Python数据分析', '1-3年', '本科以上', '10000-20000元'], ['厦门北星博辉科技有限公司', 'Python开发工程师', '1-3年', '本科以上', '8000-12000'], ['厦门匹夫有道信息技术有限公司', 'Python开发+双休', '不限经验', '大专以上', '3K-5K'], ['品尚电商', 'python工程师', '2年以上', '本科以上', '7-12万']]\n",
      "[['厦门精图信息技术有限公司', 'Python开发工程师', '不限经验', '不限学历', '6000-10000元/月'], ['上海红意坊市场营销策划事务所', 'python后端', '1-3年', '本科以上', '10000-15000元'], ['匠思崛（厦门）信息科技有限公司', 'python实习', '不限经验', '不限学历', '2千-4千'], ['厦门金投缘信息科技有限公司', 'Python数据分析', '1-3年', '本科以上', '10000-20000元'], ['厦门北星博辉科技有限公司', 'Python开发工程师', '1-3年', '本科以上', '8000-12000'], ['厦门匹夫有道信息技术有限公司', 'Python开发+双休', '不限经验', '大专以上', '3K-5K'], ['品尚电商', 'python工程师', '2年以上', '本科以上', '7-12万'], ['厦门紫竹数码科技有限公司', 'python工程师', '不限经验', '不限学历', '5000-10000元/月']]\n",
      "[['厦门精图信息技术有限公司', 'Python开发工程师', '不限经验', '不限学历', '6000-10000元/月'], ['上海红意坊市场营销策划事务所', 'python后端', '1-3年', '本科以上', '10000-15000元'], ['匠思崛（厦门）信息科技有限公司', 'python实习', '不限经验', '不限学历', '2千-4千'], ['厦门金投缘信息科技有限公司', 'Python数据分析', '1-3年', '本科以上', '10000-20000元'], ['厦门北星博辉科技有限公司', 'Python开发工程师', '1-3年', '本科以上', '8000-12000'], ['厦门匹夫有道信息技术有限公司', 'Python开发+双休', '不限经验', '大专以上', '3K-5K'], ['品尚电商', 'python工程师', '2年以上', '本科以上', '7-12万'], ['厦门紫竹数码科技有限公司', 'python工程师', '不限经验', '不限学历', '5000-10000元/月'], ['厦门点滴汇信息科技有限公司', 'Python开发工程师', '1年经验', '本科以上', '0.8-1万/月']]\n",
      "[['厦门精图信息技术有限公司', 'Python开发工程师', '不限经验', '不限学历', '6000-10000元/月'], ['上海红意坊市场营销策划事务所', 'python后端', '1-3年', '本科以上', '10000-15000元'], ['匠思崛（厦门）信息科技有限公司', 'python实习', '不限经验', '不限学历', '2千-4千'], ['厦门金投缘信息科技有限公司', 'Python数据分析', '1-3年', '本科以上', '10000-20000元'], ['厦门北星博辉科技有限公司', 'Python开发工程师', '1-3年', '本科以上', '8000-12000'], ['厦门匹夫有道信息技术有限公司', 'Python开发+双休', '不限经验', '大专以上', '3K-5K'], ['品尚电商', 'python工程师', '2年以上', '本科以上', '7-12万'], ['厦门紫竹数码科技有限公司', 'python工程师', '不限经验', '不限学历', '5000-10000元/月'], ['厦门点滴汇信息科技有限公司', 'Python开发工程师', '1年经验', '本科以上', '0.8-1万/月'], ['厦门火冶网络科技有限公司', 'Python开发工程师', '不限经验', '不限学历', '6000-12000元/月']]\n",
      "[['厦门精图信息技术有限公司', 'Python开发工程师', '不限经验', '不限学历', '6000-10000元/月'], ['上海红意坊市场营销策划事务所', 'python后端', '1-3年', '本科以上', '10000-15000元'], ['匠思崛（厦门）信息科技有限公司', 'python实习', '不限经验', '不限学历', '2千-4千'], ['厦门金投缘信息科技有限公司', 'Python数据分析', '1-3年', '本科以上', '10000-20000元'], ['厦门北星博辉科技有限公司', 'Python开发工程师', '1-3年', '本科以上', '8000-12000'], ['厦门匹夫有道信息技术有限公司', 'Python开发+双休', '不限经验', '大专以上', '3K-5K'], ['品尚电商', 'python工程师', '2年以上', '本科以上', '7-12万'], ['厦门紫竹数码科技有限公司', 'python工程师', '不限经验', '不限学历', '5000-10000元/月'], ['厦门点滴汇信息科技有限公司', 'Python开发工程师', '1年经验', '本科以上', '0.8-1万/月'], ['厦门火冶网络科技有限公司', 'Python开发工程师', '不限经验', '不限学历', '6000-12000元/月'], ['厦门星族互动网络科技有限公司', '游戏服务端python开发经理', '不限经验', '大专以上', '10000-20000元/月']]\n",
      "[['厦门精图信息技术有限公司', 'Python开发工程师', '不限经验', '不限学历', '6000-10000元/月'], ['上海红意坊市场营销策划事务所', 'python后端', '1-3年', '本科以上', '10000-15000元'], ['匠思崛（厦门）信息科技有限公司', 'python实习', '不限经验', '不限学历', '2千-4千'], ['厦门金投缘信息科技有限公司', 'Python数据分析', '1-3年', '本科以上', '10000-20000元'], ['厦门北星博辉科技有限公司', 'Python开发工程师', '1-3年', '本科以上', '8000-12000'], ['厦门匹夫有道信息技术有限公司', 'Python开发+双休', '不限经验', '大专以上', '3K-5K'], ['品尚电商', 'python工程师', '2年以上', '本科以上', '7-12万'], ['厦门紫竹数码科技有限公司', 'python工程师', '不限经验', '不限学历', '5000-10000元/月'], ['厦门点滴汇信息科技有限公司', 'Python开发工程师', '1年经验', '本科以上', '0.8-1万/月'], ['厦门火冶网络科技有限公司', 'Python开发工程师', '不限经验', '不限学历', '6000-12000元/月'], ['厦门星族互动网络科技有限公司', '游戏服务端python开发经理', '不限经验', '大专以上', '10000-20000元/月'], ['厦门星族互动网络科技有限公司', '游戏服务端python工程师', '不限经验', '大专以上', '5000-10000元/月']]\n",
      "[['厦门精图信息技术有限公司', 'Python开发工程师', '不限经验', '不限学历', '6000-10000元/月'], ['上海红意坊市场营销策划事务所', 'python后端', '1-3年', '本科以上', '10000-15000元'], ['匠思崛（厦门）信息科技有限公司', 'python实习', '不限经验', '不限学历', '2千-4千'], ['厦门金投缘信息科技有限公司', 'Python数据分析', '1-3年', '本科以上', '10000-20000元'], ['厦门北星博辉科技有限公司', 'Python开发工程师', '1-3年', '本科以上', '8000-12000'], ['厦门匹夫有道信息技术有限公司', 'Python开发+双休', '不限经验', '大专以上', '3K-5K'], ['品尚电商', 'python工程师', '2年以上', '本科以上', '7-12万'], ['厦门紫竹数码科技有限公司', 'python工程师', '不限经验', '不限学历', '5000-10000元/月'], ['厦门点滴汇信息科技有限公司', 'Python开发工程师', '1年经验', '本科以上', '0.8-1万/月'], ['厦门火冶网络科技有限公司', 'Python开发工程师', '不限经验', '不限学历', '6000-12000元/月'], ['厦门星族互动网络科技有限公司', '游戏服务端python开发经理', '不限经验', '大专以上', '10000-20000元/月'], ['厦门星族互动网络科技有限公司', '游戏服务端python工程师', '不限经验', '大专以上', '5000-10000元/月'], ['厦门星海无限科技有限公司', 'python工程师 爬虫工程师', '二年工作经验以上', '大专以上', '8000-12000元/月']]\n",
      "[['厦门精图信息技术有限公司', 'Python开发工程师', '不限经验', '不限学历', '6000-10000元/月'], ['上海红意坊市场营销策划事务所', 'python后端', '1-3年', '本科以上', '10000-15000元'], ['匠思崛（厦门）信息科技有限公司', 'python实习', '不限经验', '不限学历', '2千-4千'], ['厦门金投缘信息科技有限公司', 'Python数据分析', '1-3年', '本科以上', '10000-20000元'], ['厦门北星博辉科技有限公司', 'Python开发工程师', '1-3年', '本科以上', '8000-12000'], ['厦门匹夫有道信息技术有限公司', 'Python开发+双休', '不限经验', '大专以上', '3K-5K'], ['品尚电商', 'python工程师', '2年以上', '本科以上', '7-12万'], ['厦门紫竹数码科技有限公司', 'python工程师', '不限经验', '不限学历', '5000-10000元/月'], ['厦门点滴汇信息科技有限公司', 'Python开发工程师', '1年经验', '本科以上', '0.8-1万/月'], ['厦门火冶网络科技有限公司', 'Python开发工程师', '不限经验', '不限学历', '6000-12000元/月'], ['厦门星族互动网络科技有限公司', '游戏服务端python开发经理', '不限经验', '大专以上', '10000-20000元/月'], ['厦门星族互动网络科技有限公司', '游戏服务端python工程师', '不限经验', '大专以上', '5000-10000元/月'], ['厦门星海无限科技有限公司', 'python工程师 爬虫工程师', '二年工作经验以上', '大专以上', '8000-12000元/月'], ['厦门荣趣网络科技有限公司', 'Python后台工程师', '不限经验', '大专以上', '5000-9500元/月']]\n",
      "[['厦门精图信息技术有限公司', 'Python开发工程师', '不限经验', '不限学历', '6000-10000元/月'], ['上海红意坊市场营销策划事务所', 'python后端', '1-3年', '本科以上', '10000-15000元'], ['匠思崛（厦门）信息科技有限公司', 'python实习', '不限经验', '不限学历', '2千-4千'], ['厦门金投缘信息科技有限公司', 'Python数据分析', '1-3年', '本科以上', '10000-20000元'], ['厦门北星博辉科技有限公司', 'Python开发工程师', '1-3年', '本科以上', '8000-12000'], ['厦门匹夫有道信息技术有限公司', 'Python开发+双休', '不限经验', '大专以上', '3K-5K'], ['品尚电商', 'python工程师', '2年以上', '本科以上', '7-12万'], ['厦门紫竹数码科技有限公司', 'python工程师', '不限经验', '不限学历', '5000-10000元/月'], ['厦门点滴汇信息科技有限公司', 'Python开发工程师', '1年经验', '本科以上', '0.8-1万/月'], ['厦门火冶网络科技有限公司', 'Python开发工程师', '不限经验', '不限学历', '6000-12000元/月'], ['厦门星族互动网络科技有限公司', '游戏服务端python开发经理', '不限经验', '大专以上', '10000-20000元/月'], ['厦门星族互动网络科技有限公司', '游戏服务端python工程师', '不限经验', '大专以上', '5000-10000元/月'], ['厦门星海无限科技有限公司', 'python工程师 爬虫工程师', '二年工作经验以上', '大专以上', '8000-12000元/月'], ['厦门荣趣网络科技有限公司', 'Python后台工程师', '不限经验', '大专以上', '5000-9500元/月'], ['厦门荣趣网络科技有限公司', 'Python算法工程师', '三年工作经验以上', '博士以上', '5500-11000元/月']]\n"
     ]
    }
   ],
   "source": [
    "# 综合案例3：爬取互联网求职招聘数据\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "list_url = []  # 存储生成的从第一页到第某页的网页链接\n",
    "list_experience = []  # 存储求职经验\n",
    "list_education = []  # 存储学历\n",
    "list_money = []  # 存储工资\n",
    "list_name = []  # 存储求职公司名称\n",
    "list_job = []  # 存职位名称\n",
    "\n",
    "def GetInfos(name, city, pages):\n",
    "    # 保存从第一页到第某页的网页链接\n",
    "    for i in range(1, pages):\n",
    "        global URL\n",
    "        URL = 'https://www.jobui.com/jobs?jobKw={0}&cityKw={1}&n={2}'.format(name, city, i)\n",
    "        list_url.append(URL)\n",
    "    # 爬虫请求头信息\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50',\n",
    "    }\n",
    "    # 使用requsets获取网站html信息内容\n",
    "    wb_data = requests.get(URL, headers=headers)\n",
    "    #print(wb_data)\n",
    "    # 使用BeautifulSoup\n",
    "    soup = BeautifulSoup(wb_data.content, 'lxml')\n",
    "    # 通过使用BeautifulSoup的选取函数获取对应标签的信息内容\n",
    "    # 获取公司名称\n",
    "    names = soup.select('div > div > div > div > div > div > div > div > a.job-company-name')\n",
    "    for i in names:\n",
    "        list_name.append(i.get_text().replace('\\n', ''))\n",
    "    # 获取岗位名称\n",
    "    jobs = soup.select('div > div > div > div > div > div > div > div > a.job-name')\n",
    "    for i in jobs:\n",
    "        list_job.append(i.get_text().replace('\\n', ''))\n",
    "    # 获取岗位详情\n",
    "    for i in range(1, 17):\n",
    "        msg = soup.select('div > div > div > div > div > div > div > div > div.job-desc')\n",
    "        for m in msg:\n",
    "            info = m.get_text()\n",
    "            try:\n",
    "                info = info.split('|')\n",
    "                list_experience.append(info[0].strip())\n",
    "                list_education.append(info[1].strip())\n",
    "                list_money.append(info[2].strip())\n",
    "            except:\n",
    "                pass\n",
    "    infos = []\n",
    "    for name, job, experience, education, salary in zip(list_name, list_job, list_experience,  list_education, list_money):\n",
    "        info =[]\n",
    "        info.append(name)\n",
    "        info.append(job)\n",
    "        info.append(experience)\n",
    "        info.append(education)\n",
    "        info.append(salary)\n",
    "        infos.append(info)\n",
    "        print(infos)\n",
    "    return infos\n",
    "\n",
    "def main():\n",
    "    city = \"厦门\"\n",
    "    job = \"python\"\n",
    "    infos = GetInfos(job,city , 4)\n",
    "    filename = '厦门' + '_' + 'python' + '.csv'\n",
    "    headers = ['公司名称', '职位名称', '经验', '学历','薪资']\n",
    "    # 获取所有职位信息，写入csv文件\n",
    "    index = [i for i in range(1, len(infos)+1)]\n",
    "    df2 = pd.DataFrame(infos, columns=headers,index=index)\n",
    "    filename = city + \"_\"+job +\".csv\"\n",
    "    pd.DataFrame.to_csv(df2, filename)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
